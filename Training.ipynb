{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10137452,"sourceType":"datasetVersion","datasetId":6256532}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the paths to your video and caption directories\nvideo_folder = '/kaggle/input/soccer/content/drive/MyDrive/SoccerNet_Captions/TrimmedVideos'  # Path to video folder\ncaption_folder = '/kaggle/input/soccer/content/drive/MyDrive/SoccerNet_Captions/TrimmedCaptions'  # Path to caption folder\n\n# Get a list of video files (assuming they have extensions like .mp4)\nvideo_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n\n# Create a list to store video paths and captions\nvideo_paths = []\ncaptions = []\n\n# Iterate through each video file\nfor video_file in video_files:\n    # Get the corresponding caption file name (same name, but with .txt extension)\n    caption_file = video_file.replace('.mp4', '.txt')\n    \n    # Check if the caption file exists\n    if caption_file in os.listdir(caption_folder):\n        # Construct full path to video file\n        video_paths.append(os.path.join(video_folder, video_file))\n        \n        # Read the caption from the caption file\n        caption_path = os.path.join(caption_folder, caption_file)\n        with open(caption_path, 'r') as file:\n            caption = file.read().strip()  # Read and remove any extra spaces or newlines\n        captions.append(caption)\n\n# Create a DataFrame to store the video paths and captions\ndf = pd.DataFrame({\n    'video_path': video_paths,\n    'caption': captions\n})\n\n# Display the DataFrame\ndf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:48:36.219842Z","iopub.execute_input":"2024-12-08T16:48:36.220232Z","iopub.status.idle":"2024-12-08T16:48:36.915842Z","shell.execute_reply.started":"2024-12-08T16:48:36.220185Z","shell.execute_reply":"2024-12-08T16:48:36.914896Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                            video_path  \\\n0    /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n1    /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n2    /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n3    /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n4    /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n..                                                 ...   \n610  /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n611  /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n612  /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n613  /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n614  /kaggle/input/soccer/content/drive/MyDrive/Soc...   \n\n                                               caption  \n0    Salomon Rondon (West Brom) receives a precise ...  \n1    Aaron Cresswell (West Ham) swings a cross into...  \n2    Gabriel Obertan (Newcastle Utd) fails to find ...  \n3    Substitution. Roberto Firmino did his best and...  \n4    Aaron Cresswell (West Ham) tries to slide the ...  \n..                                                 ...  \n610  Cesc Fabregas (Chelsea) takes the free kick bu...  \n611  Dusan Tadic (Southampton) flights in the cross...  \n612  Wayne Rooney (Manchester United) escapes witho...  \n613  Cesc Fabregas (Chelsea) steps up to take the c...  \n614  Manuel Lanzini (West Ham) creates a good chanc...  \n\n[615 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_path</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Salomon Rondon (West Brom) receives a precise ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Aaron Cresswell (West Ham) swings a cross into...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Gabriel Obertan (Newcastle Utd) fails to find ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Substitution. Roberto Firmino did his best and...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Aaron Cresswell (West Ham) tries to slide the ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Cesc Fabregas (Chelsea) takes the free kick bu...</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Dusan Tadic (Southampton) flights in the cross...</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Wayne Rooney (Manchester United) escapes witho...</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Cesc Fabregas (Chelsea) steps up to take the c...</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>/kaggle/input/soccer/content/drive/MyDrive/Soc...</td>\n      <td>Manuel Lanzini (West Ham) creates a good chanc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>615 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"pip install av","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:44:33.490421Z","iopub.execute_input":"2024-12-08T16:44:33.490673Z","iopub.status.idle":"2024-12-08T16:44:41.755678Z","shell.execute_reply.started":"2024-12-08T16:44:33.490649Z","shell.execute_reply":"2024-12-08T16:44:41.754288Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (14.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoImageProcessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:44:33.484276Z","iopub.execute_input":"2024-12-08T16:44:33.485014Z","iopub.status.idle":"2024-12-08T16:44:33.489484Z","shell.execute_reply.started":"2024-12-08T16:44:33.484946Z","shell.execute_reply":"2024-12-08T16:44:33.488653Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoImageProcessor\nimport av\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Initialize tokenizer and image processor\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n\n# Assign a padding token if not defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Define custom dataset\nclass VideoCaptionDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_processor, device, clip_len=16, max_length=20):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.device = device\n        self.clip_len = clip_len\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Get video path and caption\n        video_path = self.dataframe.iloc[idx]['video_path']\n        caption = self.dataframe.iloc[idx]['caption']\n        \n        # Process video frames\n        container = av.open(video_path)\n        seg_len = container.streams.video[0].frames\n        indices = set(np.linspace(0, seg_len, num=self.clip_len, endpoint=False).astype(np.int64))\n        frames = []\n\n        container.seek(0)\n        for i, frame in enumerate(container.decode(video=0)):\n            if i in indices:\n                frames.append(frame.to_ndarray(format=\"rgb24\"))\n\n        # Prepare pixel values using the image processor\n        pixel_values = self.image_processor(frames, return_tensors=\"pt\").pixel_values.to(self.device)\n\n        # Tokenize caption with padding and truncation\n        inputs = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_length, truncation=True)\n        labels = inputs.input_ids.to(self.device)\n\n        # Mask padding tokens in labels\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return torch.squeeze(pixel_values), torch.squeeze(labels), caption\n\n\n# Train-test split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize the dataset and DataLoader for train and test\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Train DataLoader\ntrain_dataset = VideoCaptionDataset(train_df, tokenizer, image_processor, device)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Test DataLoader\ntest_dataset = VideoCaptionDataset(test_df, tokenizer, image_processor, device)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Example usage of DataLoader (train)\nfor pixel_values, labels, _ in train_dataloader:\n    print(\"Train - Pixel values shape:\", pixel_values.shape)\n    print(\"Train - Labels shape:\", labels.shape)\n    break  # Just to print one batch for now\n\n# Example usage of DataLoader (test)\nfor pixel_values, labels, _ in test_dataloader:\n    print(\"Test - Pixel values shape:\", pixel_values.shape)\n    print(\"Test - Labels shape:\", labels.shape)\n    break  # Just to print one batch for now\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:44:41.761928Z","iopub.execute_input":"2024-12-08T16:44:41.762405Z","iopub.status.idle":"2024-12-08T16:44:49.737150Z","shell.execute_reply.started":"2024-12-08T16:44:41.762368Z","shell.execute_reply":"2024-12-08T16:44:49.736227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d70227af8c443abf4569932dc8d14e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7032f4c527f247619186dd6b0dc1a4df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23092772a47a46ee85f7b5cab63a6416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50fd996888ac4d57858cf2124add605a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c31a0ddf9d14d50b449e3d6f7802913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"259894d8c3d744b794525f323ca4a01f"}},"metadata":{}},{"name":"stdout","text":"Train - Pixel values shape: torch.Size([8, 16, 3, 224, 224])\nTrain - Labels shape: torch.Size([8, 20])\nTest - Pixel values shape: torch.Size([8, 16, 3, 224, 224])\nTest - Labels shape: torch.Size([8, 20])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"Neleac/timesformer-gpt2-video-captioning\").to(device)\n\n# Freeze encoder parameters\nfor param in model.encoder.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:44:49.738433Z","iopub.execute_input":"2024-12-08T16:44:49.739498Z","iopub.status.idle":"2024-12-08T16:45:03.250217Z","shell.execute_reply.started":"2024-12-08T16:44:49.739442Z","shell.execute_reply":"2024-12-08T16:45:03.249383Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/41.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1793690e9ad74cd7a3d43fc1252ae642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abdcc569eff54fceb377f230877e5d64"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.timesformer.modeling_timesformer.TimesformerModel'> is overwritten by shared encoder config: TimesformerConfig {\n  \"_name_or_path\": \"facebook/timesformer-base-finetuned-k600\",\n  \"architectures\": [\n    \"TimesformerForVideoClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"attention_type\": \"divided_space_time\",\n  \"drop_path_rate\": 0,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"abseiling\",\n    \"1\": \"acting in play\",\n    \"2\": \"adjusting glasses\",\n    \"3\": \"air drumming\",\n    \"4\": \"alligator wrestling\",\n    \"5\": \"answering questions\",\n    \"6\": \"applauding\",\n    \"7\": \"applying cream\",\n    \"8\": \"archaeological excavation\",\n    \"9\": \"archery\",\n    \"10\": \"arguing\",\n    \"11\": \"arm wrestling\",\n    \"12\": \"arranging flowers\",\n    \"13\": \"assembling bicycle\",\n    \"14\": \"assembling computer\",\n    \"15\": \"attending conference\",\n    \"16\": \"auctioning\",\n    \"17\": \"backflip (human)\",\n    \"18\": \"baking cookies\",\n    \"19\": \"bandaging\",\n    \"20\": \"barbequing\",\n    \"21\": \"bartending\",\n    \"22\": \"base jumping\",\n    \"23\": \"bathing dog\",\n    \"24\": \"battle rope training\",\n    \"25\": \"beatboxing\",\n    \"26\": \"bee keeping\",\n    \"27\": \"belly dancing\",\n    \"28\": \"bench pressing\",\n    \"29\": \"bending back\",\n    \"30\": \"bending metal\",\n    \"31\": \"biking through snow\",\n    \"32\": \"blasting sand\",\n    \"33\": \"blowdrying hair\",\n    \"34\": \"blowing bubble gum\",\n    \"35\": \"blowing glass\",\n    \"36\": \"blowing leaves\",\n    \"37\": \"blowing nose\",\n    \"38\": \"blowing out candles\",\n    \"39\": \"bobsledding\",\n    \"40\": \"bodysurfing\",\n    \"41\": \"bookbinding\",\n    \"42\": \"bottling\",\n    \"43\": \"bouncing on bouncy castle\",\n    \"44\": \"bouncing on trampoline\",\n    \"45\": \"bowling\",\n    \"46\": \"braiding hair\",\n    \"47\": \"breading or breadcrumbing\",\n    \"48\": \"breakdancing\",\n    \"49\": \"breaking boards\",\n    \"50\": \"breathing fire\",\n    \"51\": \"brush painting\",\n    \"52\": \"brushing hair\",\n    \"53\": \"brushing teeth\",\n    \"54\": \"building cabinet\",\n    \"55\": \"building lego\",\n    \"56\": \"building sandcastle\",\n    \"57\": \"building shed\",\n    \"58\": \"bull fighting\",\n    \"59\": \"bulldozing\",\n    \"60\": \"bungee jumping\",\n    \"61\": \"burping\",\n    \"62\": \"busking\",\n    \"63\": \"calculating\",\n    \"64\": \"calligraphy\",\n    \"65\": \"canoeing or kayaking\",\n    \"66\": \"capoeira\",\n    \"67\": \"capsizing\",\n    \"68\": \"card stacking\",\n    \"69\": \"card throwing\",\n    \"70\": \"carrying baby\",\n    \"71\": \"cartwheeling\",\n    \"72\": \"carving ice\",\n    \"73\": \"carving pumpkin\",\n    \"74\": \"casting fishing line\",\n    \"75\": \"catching fish\",\n    \"76\": \"catching or throwing baseball\",\n    \"77\": \"catching or throwing frisbee\",\n    \"78\": \"catching or throwing softball\",\n    \"79\": \"celebrating\",\n    \"80\": \"changing gear in car\",\n    \"81\": \"changing oil\",\n    \"82\": \"changing wheel (not on bike)\",\n    \"83\": \"checking tires\",\n    \"84\": \"cheerleading\",\n    \"85\": \"chewing gum\",\n    \"86\": \"chiseling stone\",\n    \"87\": \"chiseling wood\",\n    \"88\": \"chopping meat\",\n    \"89\": \"chopping vegetables\",\n    \"90\": \"chopping wood\",\n    \"91\": \"clam digging\",\n    \"92\": \"clapping\",\n    \"93\": \"clay pottery making\",\n    \"94\": \"clean and jerk\",\n    \"95\": \"cleaning gutters\",\n    \"96\": \"cleaning pool\",\n    \"97\": \"cleaning shoes\",\n    \"98\": \"cleaning toilet\",\n    \"99\": \"cleaning windows\",\n    \"100\": \"climbing a rope\",\n    \"101\": \"climbing ladder\",\n    \"102\": \"climbing tree\",\n    \"103\": \"coloring in\",\n    \"104\": \"combing hair\",\n    \"105\": \"contact juggling\",\n    \"106\": \"contorting\",\n    \"107\": \"cooking egg\",\n    \"108\": \"cooking on campfire\",\n    \"109\": \"cooking sausages (not on barbeque)\",\n    \"110\": \"cooking scallops\",\n    \"111\": \"cosplaying\",\n    \"112\": \"counting money\",\n    \"113\": \"country line dancing\",\n    \"114\": \"cracking back\",\n    \"115\": \"cracking knuckles\",\n    \"116\": \"cracking neck\",\n    \"117\": \"crawling baby\",\n    \"118\": \"crossing eyes\",\n    \"119\": \"crossing river\",\n    \"120\": \"crying\",\n    \"121\": \"cumbia\",\n    \"122\": \"curling (sport)\",\n    \"123\": \"curling hair\",\n    \"124\": \"cutting apple\",\n    \"125\": \"cutting nails\",\n    \"126\": \"cutting orange\",\n    \"127\": \"cutting pineapple\",\n    \"128\": \"cutting watermelon\",\n    \"129\": \"dancing ballet\",\n    \"130\": \"dancing charleston\",\n    \"131\": \"dancing gangnam style\",\n    \"132\": \"dancing macarena\",\n    \"133\": \"deadlifting\",\n    \"134\": \"decorating the christmas tree\",\n    \"135\": \"delivering mail\",\n    \"136\": \"dining\",\n    \"137\": \"directing traffic\",\n    \"138\": \"disc golfing\",\n    \"139\": \"diving cliff\",\n    \"140\": \"docking boat\",\n    \"141\": \"dodgeball\",\n    \"142\": \"doing aerobics\",\n    \"143\": \"doing jigsaw puzzle\",\n    \"144\": \"doing laundry\",\n    \"145\": \"doing nails\",\n    \"146\": \"drawing\",\n    \"147\": \"dribbling basketball\",\n    \"148\": \"drinking shots\",\n    \"149\": \"driving car\",\n    \"150\": \"driving tractor\",\n    \"151\": \"drooling\",\n    \"152\": \"drop kicking\",\n    \"153\": \"drumming fingers\",\n    \"154\": \"dumpster diving\",\n    \"155\": \"dunking basketball\",\n    \"156\": \"dyeing eyebrows\",\n    \"157\": \"dyeing hair\",\n    \"158\": \"eating burger\",\n    \"159\": \"eating cake\",\n    \"160\": \"eating carrots\",\n    \"161\": \"eating chips\",\n    \"162\": \"eating doughnuts\",\n    \"163\": \"eating hotdog\",\n    \"164\": \"eating ice cream\",\n    \"165\": \"eating spaghetti\",\n    \"166\": \"eating watermelon\",\n    \"167\": \"egg hunting\",\n    \"168\": \"embroidering\",\n    \"169\": \"exercising with an exercise ball\",\n    \"170\": \"extinguishing fire\",\n    \"171\": \"faceplanting\",\n    \"172\": \"falling off bike\",\n    \"173\": \"falling off chair\",\n    \"174\": \"feeding birds\",\n    \"175\": \"feeding fish\",\n    \"176\": \"feeding goats\",\n    \"177\": \"fencing (sport)\",\n    \"178\": \"fidgeting\",\n    \"179\": \"finger snapping\",\n    \"180\": \"fixing bicycle\",\n    \"181\": \"fixing hair\",\n    \"182\": \"flint knapping\",\n    \"183\": \"flipping pancake\",\n    \"184\": \"fly tying\",\n    \"185\": \"flying kite\",\n    \"186\": \"folding clothes\",\n    \"187\": \"folding napkins\",\n    \"188\": \"folding paper\",\n    \"189\": \"front raises\",\n    \"190\": \"frying vegetables\",\n    \"191\": \"geocaching\",\n    \"192\": \"getting a haircut\",\n    \"193\": \"getting a piercing\",\n    \"194\": \"getting a tattoo\",\n    \"195\": \"giving or receiving award\",\n    \"196\": \"gold panning\",\n    \"197\": \"golf chipping\",\n    \"198\": \"golf driving\",\n    \"199\": \"golf putting\",\n    \"200\": \"gospel singing in church\",\n    \"201\": \"grinding meat\",\n    \"202\": \"grooming dog\",\n    \"203\": \"grooming horse\",\n    \"204\": \"gymnastics tumbling\",\n    \"205\": \"hammer throw\",\n    \"206\": \"hand washing clothes\",\n    \"207\": \"head stand\",\n    \"208\": \"headbanging\",\n    \"209\": \"headbutting\",\n    \"210\": \"high jump\",\n    \"211\": \"high kick\",\n    \"212\": \"historical reenactment\",\n    \"213\": \"hitting baseball\",\n    \"214\": \"hockey stop\",\n    \"215\": \"holding snake\",\n    \"216\": \"home roasting coffee\",\n    \"217\": \"hopscotch\",\n    \"218\": \"hoverboarding\",\n    \"219\": \"huddling\",\n    \"220\": \"hugging (not baby)\",\n    \"221\": \"hugging baby\",\n    \"222\": \"hula hooping\",\n    \"223\": \"hurdling\",\n    \"224\": \"hurling (sport)\",\n    \"225\": \"ice climbing\",\n    \"226\": \"ice fishing\",\n    \"227\": \"ice skating\",\n    \"228\": \"ice swimming\",\n    \"229\": \"inflating balloons\",\n    \"230\": \"installing carpet\",\n    \"231\": \"ironing\",\n    \"232\": \"ironing hair\",\n    \"233\": \"javelin throw\",\n    \"234\": \"jaywalking\",\n    \"235\": \"jetskiing\",\n    \"236\": \"jogging\",\n    \"237\": \"juggling balls\",\n    \"238\": \"juggling fire\",\n    \"239\": \"juggling soccer ball\",\n    \"240\": \"jumping bicycle\",\n    \"241\": \"jumping into pool\",\n    \"242\": \"jumping jacks\",\n    \"243\": \"jumpstyle dancing\",\n    \"244\": \"karaoke\",\n    \"245\": \"kicking field goal\",\n    \"246\": \"kicking soccer ball\",\n    \"247\": \"kissing\",\n    \"248\": \"kitesurfing\",\n    \"249\": \"knitting\",\n    \"250\": \"krumping\",\n    \"251\": \"land sailing\",\n    \"252\": \"laughing\",\n    \"253\": \"lawn mower racing\",\n    \"254\": \"laying bricks\",\n    \"255\": \"laying concrete\",\n    \"256\": \"laying stone\",\n    \"257\": \"laying tiles\",\n    \"258\": \"leatherworking\",\n    \"259\": \"licking\",\n    \"260\": \"lifting hat\",\n    \"261\": \"lighting fire\",\n    \"262\": \"lock picking\",\n    \"263\": \"long jump\",\n    \"264\": \"longboarding\",\n    \"265\": \"looking at phone\",\n    \"266\": \"luge\",\n    \"267\": \"lunge\",\n    \"268\": \"making a cake\",\n    \"269\": \"making a sandwich\",\n    \"270\": \"making balloon shapes\",\n    \"271\": \"making bubbles\",\n    \"272\": \"making cheese\",\n    \"273\": \"making horseshoes\",\n    \"274\": \"making jewelry\",\n    \"275\": \"making paper aeroplanes\",\n    \"276\": \"making pizza\",\n    \"277\": \"making snowman\",\n    \"278\": \"making sushi\",\n    \"279\": \"making tea\",\n    \"280\": \"making the bed\",\n    \"281\": \"marching\",\n    \"282\": \"marriage proposal\",\n    \"283\": \"massaging back\",\n    \"284\": \"massaging feet\",\n    \"285\": \"massaging legs\",\n    \"286\": \"massaging neck\",\n    \"287\": \"massaging person's head\",\n    \"288\": \"milking cow\",\n    \"289\": \"moon walking\",\n    \"290\": \"mopping floor\",\n    \"291\": \"mosh pit dancing\",\n    \"292\": \"motorcycling\",\n    \"293\": \"mountain climber (exercise)\",\n    \"294\": \"moving furniture\",\n    \"295\": \"mowing lawn\",\n    \"296\": \"mushroom foraging\",\n    \"297\": \"needle felting\",\n    \"298\": \"news anchoring\",\n    \"299\": \"opening bottle (not wine)\",\n    \"300\": \"opening door\",\n    \"301\": \"opening present\",\n    \"302\": \"opening refrigerator\",\n    \"303\": \"opening wine bottle\",\n    \"304\": \"packing\",\n    \"305\": \"paragliding\",\n    \"306\": \"parasailing\",\n    \"307\": \"parkour\",\n    \"308\": \"passing American football (in game)\",\n    \"309\": \"passing american football (not in game)\",\n    \"310\": \"passing soccer ball\",\n    \"311\": \"peeling apples\",\n    \"312\": \"peeling potatoes\",\n    \"313\": \"person collecting garbage\",\n    \"314\": \"petting animal (not cat)\",\n    \"315\": \"petting cat\",\n    \"316\": \"photobombing\",\n    \"317\": \"photocopying\",\n    \"318\": \"picking fruit\",\n    \"319\": \"pillow fight\",\n    \"320\": \"pinching\",\n    \"321\": \"pirouetting\",\n    \"322\": \"planing wood\",\n    \"323\": \"planting trees\",\n    \"324\": \"plastering\",\n    \"325\": \"playing accordion\",\n    \"326\": \"playing badminton\",\n    \"327\": \"playing bagpipes\",\n    \"328\": \"playing basketball\",\n    \"329\": \"playing bass guitar\",\n    \"330\": \"playing beer pong\",\n    \"331\": \"playing blackjack\",\n    \"332\": \"playing cello\",\n    \"333\": \"playing chess\",\n    \"334\": \"playing clarinet\",\n    \"335\": \"playing controller\",\n    \"336\": \"playing cricket\",\n    \"337\": \"playing cymbals\",\n    \"338\": \"playing darts\",\n    \"339\": \"playing didgeridoo\",\n    \"340\": \"playing dominoes\",\n    \"341\": \"playing drums\",\n    \"342\": \"playing field hockey\",\n    \"343\": \"playing flute\",\n    \"344\": \"playing gong\",\n    \"345\": \"playing guitar\",\n    \"346\": \"playing hand clapping games\",\n    \"347\": \"playing harmonica\",\n    \"348\": \"playing harp\",\n    \"349\": \"playing ice hockey\",\n    \"350\": \"playing keyboard\",\n    \"351\": \"playing kickball\",\n    \"352\": \"playing laser tag\",\n    \"353\": \"playing lute\",\n    \"354\": \"playing maracas\",\n    \"355\": \"playing marbles\",\n    \"356\": \"playing monopoly\",\n    \"357\": \"playing netball\",\n    \"358\": \"playing ocarina\",\n    \"359\": \"playing organ\",\n    \"360\": \"playing paintball\",\n    \"361\": \"playing pan pipes\",\n    \"362\": \"playing piano\",\n    \"363\": \"playing pinball\",\n    \"364\": \"playing ping pong\",\n    \"365\": \"playing poker\",\n    \"366\": \"playing polo\",\n    \"367\": \"playing recorder\",\n    \"368\": \"playing rubiks cube\",\n    \"369\": \"playing saxophone\",\n    \"370\": \"playing scrabble\",\n    \"371\": \"playing squash or racquetball\",\n    \"372\": \"playing tennis\",\n    \"373\": \"playing trombone\",\n    \"374\": \"playing trumpet\",\n    \"375\": \"playing ukulele\",\n    \"376\": \"playing violin\",\n    \"377\": \"playing volleyball\",\n    \"378\": \"playing with trains\",\n    \"379\": \"playing xylophone\",\n    \"380\": \"poking bellybutton\",\n    \"381\": \"pole vault\",\n    \"382\": \"polishing metal\",\n    \"383\": \"popping balloons\",\n    \"384\": \"pouring beer\",\n    \"385\": \"preparing salad\",\n    \"386\": \"presenting weather forecast\",\n    \"387\": \"pull ups\",\n    \"388\": \"pumping fist\",\n    \"389\": \"pumping gas\",\n    \"390\": \"punching bag\",\n    \"391\": \"punching person (boxing)\",\n    \"392\": \"push up\",\n    \"393\": \"pushing car\",\n    \"394\": \"pushing cart\",\n    \"395\": \"pushing wheelbarrow\",\n    \"396\": \"pushing wheelchair\",\n    \"397\": \"putting in contact lenses\",\n    \"398\": \"putting on eyeliner\",\n    \"399\": \"putting on foundation\",\n    \"400\": \"putting on lipstick\",\n    \"401\": \"putting on mascara\",\n    \"402\": \"putting on sari\",\n    \"403\": \"putting on shoes\",\n    \"404\": \"raising eyebrows\",\n    \"405\": \"reading book\",\n    \"406\": \"reading newspaper\",\n    \"407\": \"recording music\",\n    \"408\": \"repairing puncture\",\n    \"409\": \"riding a bike\",\n    \"410\": \"riding camel\",\n    \"411\": \"riding elephant\",\n    \"412\": \"riding mechanical bull\",\n    \"413\": \"riding mule\",\n    \"414\": \"riding or walking with horse\",\n    \"415\": \"riding scooter\",\n    \"416\": \"riding snow blower\",\n    \"417\": \"riding unicycle\",\n    \"418\": \"ripping paper\",\n    \"419\": \"roasting marshmallows\",\n    \"420\": \"roasting pig\",\n    \"421\": \"robot dancing\",\n    \"422\": \"rock climbing\",\n    \"423\": \"rock scissors paper\",\n    \"424\": \"roller skating\",\n    \"425\": \"rolling pastry\",\n    \"426\": \"rope pushdown\",\n    \"427\": \"running on treadmill\",\n    \"428\": \"sailing\",\n    \"429\": \"salsa dancing\",\n    \"430\": \"sanding floor\",\n    \"431\": \"sausage making\",\n    \"432\": \"sawing wood\",\n    \"433\": \"scrambling eggs\",\n    \"434\": \"scrapbooking\",\n    \"435\": \"scrubbing face\",\n    \"436\": \"scuba diving\",\n    \"437\": \"separating eggs\",\n    \"438\": \"setting table\",\n    \"439\": \"sewing\",\n    \"440\": \"shaking hands\",\n    \"441\": \"shaking head\",\n    \"442\": \"shaping bread dough\",\n    \"443\": \"sharpening knives\",\n    \"444\": \"sharpening pencil\",\n    \"445\": \"shaving head\",\n    \"446\": \"shaving legs\",\n    \"447\": \"shearing sheep\",\n    \"448\": \"shining flashlight\",\n    \"449\": \"shining shoes\",\n    \"450\": \"shooting basketball\",\n    \"451\": \"shooting goal (soccer)\",\n    \"452\": \"shopping\",\n    \"453\": \"shot put\",\n    \"454\": \"shoveling snow\",\n    \"455\": \"shucking oysters\",\n    \"456\": \"shuffling cards\",\n    \"457\": \"shuffling feet\",\n    \"458\": \"side kick\",\n    \"459\": \"sign language interpreting\",\n    \"460\": \"singing\",\n    \"461\": \"sipping cup\",\n    \"462\": \"situp\",\n    \"463\": \"skateboarding\",\n    \"464\": \"ski jumping\",\n    \"465\": \"skiing crosscountry\",\n    \"466\": \"skiing mono\",\n    \"467\": \"skiing slalom\",\n    \"468\": \"skipping rope\",\n    \"469\": \"skipping stone\",\n    \"470\": \"skydiving\",\n    \"471\": \"slacklining\",\n    \"472\": \"slapping\",\n    \"473\": \"sled dog racing\",\n    \"474\": \"sleeping\",\n    \"475\": \"smashing\",\n    \"476\": \"smelling feet\",\n    \"477\": \"smoking\",\n    \"478\": \"smoking hookah\",\n    \"479\": \"smoking pipe\",\n    \"480\": \"snatch weight lifting\",\n    \"481\": \"sneezing\",\n    \"482\": \"snorkeling\",\n    \"483\": \"snowboarding\",\n    \"484\": \"snowkiting\",\n    \"485\": \"snowmobiling\",\n    \"486\": \"somersaulting\",\n    \"487\": \"spelunking\",\n    \"488\": \"spinning poi\",\n    \"489\": \"spray painting\",\n    \"490\": \"springboard diving\",\n    \"491\": \"square dancing\",\n    \"492\": \"squat\",\n    \"493\": \"standing on hands\",\n    \"494\": \"staring\",\n    \"495\": \"steer roping\",\n    \"496\": \"sticking tongue out\",\n    \"497\": \"stomping grapes\",\n    \"498\": \"stretching arm\",\n    \"499\": \"stretching leg\",\n    \"500\": \"sucking lolly\",\n    \"501\": \"surfing crowd\",\n    \"502\": \"surfing water\",\n    \"503\": \"sweeping floor\",\n    \"504\": \"swimming backstroke\",\n    \"505\": \"swimming breast stroke\",\n    \"506\": \"swimming butterfly stroke\",\n    \"507\": \"swimming front crawl\",\n    \"508\": \"swing dancing\",\n    \"509\": \"swinging baseball bat\",\n    \"510\": \"swinging on something\",\n    \"511\": \"sword fighting\",\n    \"512\": \"sword swallowing\",\n    \"513\": \"tackling\",\n    \"514\": \"tagging graffiti\",\n    \"515\": \"tai chi\",\n    \"516\": \"talking on cell phone\",\n    \"517\": \"tango dancing\",\n    \"518\": \"tap dancing\",\n    \"519\": \"tapping guitar\",\n    \"520\": \"tapping pen\",\n    \"521\": \"tasting beer\",\n    \"522\": \"tasting food\",\n    \"523\": \"tasting wine\",\n    \"524\": \"testifying\",\n    \"525\": \"texting\",\n    \"526\": \"threading needle\",\n    \"527\": \"throwing axe\",\n    \"528\": \"throwing ball (not baseball or American football)\",\n    \"529\": \"throwing discus\",\n    \"530\": \"throwing knife\",\n    \"531\": \"throwing snowballs\",\n    \"532\": \"throwing tantrum\",\n    \"533\": \"throwing water balloon\",\n    \"534\": \"tickling\",\n    \"535\": \"tie dying\",\n    \"536\": \"tightrope walking\",\n    \"537\": \"tiptoeing\",\n    \"538\": \"tobogganing\",\n    \"539\": \"tossing coin\",\n    \"540\": \"training dog\",\n    \"541\": \"trapezing\",\n    \"542\": \"trimming or shaving beard\",\n    \"543\": \"trimming shrubs\",\n    \"544\": \"trimming trees\",\n    \"545\": \"triple jump\",\n    \"546\": \"twiddling fingers\",\n    \"547\": \"tying bow tie\",\n    \"548\": \"tying knot (not on a tie)\",\n    \"549\": \"tying necktie\",\n    \"550\": \"tying shoe laces\",\n    \"551\": \"unboxing\",\n    \"552\": \"unloading truck\",\n    \"553\": \"using a microscope\",\n    \"554\": \"using a paint roller\",\n    \"555\": \"using a power drill\",\n    \"556\": \"using a sledge hammer\",\n    \"557\": \"using a wrench\",\n    \"558\": \"using atm\",\n    \"559\": \"using bagging machine\",\n    \"560\": \"using circular saw\",\n    \"561\": \"using inhaler\",\n    \"562\": \"using puppets\",\n    \"563\": \"using remote controller (not gaming)\",\n    \"564\": \"using segway\",\n    \"565\": \"vacuuming floor\",\n    \"566\": \"visiting the zoo\",\n    \"567\": \"wading through mud\",\n    \"568\": \"wading through water\",\n    \"569\": \"waiting in line\",\n    \"570\": \"waking up\",\n    \"571\": \"walking the dog\",\n    \"572\": \"walking through snow\",\n    \"573\": \"washing dishes\",\n    \"574\": \"washing feet\",\n    \"575\": \"washing hair\",\n    \"576\": \"washing hands\",\n    \"577\": \"watching tv\",\n    \"578\": \"water skiing\",\n    \"579\": \"water sliding\",\n    \"580\": \"watering plants\",\n    \"581\": \"waving hand\",\n    \"582\": \"waxing back\",\n    \"583\": \"waxing chest\",\n    \"584\": \"waxing eyebrows\",\n    \"585\": \"waxing legs\",\n    \"586\": \"weaving basket\",\n    \"587\": \"weaving fabric\",\n    \"588\": \"welding\",\n    \"589\": \"whistling\",\n    \"590\": \"windsurfing\",\n    \"591\": \"winking\",\n    \"592\": \"wood burning (art)\",\n    \"593\": \"wrapping present\",\n    \"594\": \"wrestling\",\n    \"595\": \"writing\",\n    \"596\": \"yarn spinning\",\n    \"597\": \"yawning\",\n    \"598\": \"yoga\",\n    \"599\": \"zumba\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"abseiling\": 0,\n    \"acting in play\": 1,\n    \"adjusting glasses\": 2,\n    \"air drumming\": 3,\n    \"alligator wrestling\": 4,\n    \"answering questions\": 5,\n    \"applauding\": 6,\n    \"applying cream\": 7,\n    \"archaeological excavation\": 8,\n    \"archery\": 9,\n    \"arguing\": 10,\n    \"arm wrestling\": 11,\n    \"arranging flowers\": 12,\n    \"assembling bicycle\": 13,\n    \"assembling computer\": 14,\n    \"attending conference\": 15,\n    \"auctioning\": 16,\n    \"backflip (human)\": 17,\n    \"baking cookies\": 18,\n    \"bandaging\": 19,\n    \"barbequing\": 20,\n    \"bartending\": 21,\n    \"base jumping\": 22,\n    \"bathing dog\": 23,\n    \"battle rope training\": 24,\n    \"beatboxing\": 25,\n    \"bee keeping\": 26,\n    \"belly dancing\": 27,\n    \"bench pressing\": 28,\n    \"bending back\": 29,\n    \"bending metal\": 30,\n    \"biking through snow\": 31,\n    \"blasting sand\": 32,\n    \"blowdrying hair\": 33,\n    \"blowing bubble gum\": 34,\n    \"blowing glass\": 35,\n    \"blowing leaves\": 36,\n    \"blowing nose\": 37,\n    \"blowing out candles\": 38,\n    \"bobsledding\": 39,\n    \"bodysurfing\": 40,\n    \"bookbinding\": 41,\n    \"bottling\": 42,\n    \"bouncing on bouncy castle\": 43,\n    \"bouncing on trampoline\": 44,\n    \"bowling\": 45,\n    \"braiding hair\": 46,\n    \"breading or breadcrumbing\": 47,\n    \"breakdancing\": 48,\n    \"breaking boards\": 49,\n    \"breathing fire\": 50,\n    \"brush painting\": 51,\n    \"brushing hair\": 52,\n    \"brushing teeth\": 53,\n    \"building cabinet\": 54,\n    \"building lego\": 55,\n    \"building sandcastle\": 56,\n    \"building shed\": 57,\n    \"bull fighting\": 58,\n    \"bulldozing\": 59,\n    \"bungee jumping\": 60,\n    \"burping\": 61,\n    \"busking\": 62,\n    \"calculating\": 63,\n    \"calligraphy\": 64,\n    \"canoeing or kayaking\": 65,\n    \"capoeira\": 66,\n    \"capsizing\": 67,\n    \"card stacking\": 68,\n    \"card throwing\": 69,\n    \"carrying baby\": 70,\n    \"cartwheeling\": 71,\n    \"carving ice\": 72,\n    \"carving pumpkin\": 73,\n    \"casting fishing line\": 74,\n    \"catching fish\": 75,\n    \"catching or throwing baseball\": 76,\n    \"catching or throwing frisbee\": 77,\n    \"catching or throwing softball\": 78,\n    \"celebrating\": 79,\n    \"changing gear in car\": 80,\n    \"changing oil\": 81,\n    \"changing wheel (not on bike)\": 82,\n    \"checking tires\": 83,\n    \"cheerleading\": 84,\n    \"chewing gum\": 85,\n    \"chiseling stone\": 86,\n    \"chiseling wood\": 87,\n    \"chopping meat\": 88,\n    \"chopping vegetables\": 89,\n    \"chopping wood\": 90,\n    \"clam digging\": 91,\n    \"clapping\": 92,\n    \"clay pottery making\": 93,\n    \"clean and jerk\": 94,\n    \"cleaning gutters\": 95,\n    \"cleaning pool\": 96,\n    \"cleaning shoes\": 97,\n    \"cleaning toilet\": 98,\n    \"cleaning windows\": 99,\n    \"climbing a rope\": 100,\n    \"climbing ladder\": 101,\n    \"climbing tree\": 102,\n    \"coloring in\": 103,\n    \"combing hair\": 104,\n    \"contact juggling\": 105,\n    \"contorting\": 106,\n    \"cooking egg\": 107,\n    \"cooking on campfire\": 108,\n    \"cooking sausages (not on barbeque)\": 109,\n    \"cooking scallops\": 110,\n    \"cosplaying\": 111,\n    \"counting money\": 112,\n    \"country line dancing\": 113,\n    \"cracking back\": 114,\n    \"cracking knuckles\": 115,\n    \"cracking neck\": 116,\n    \"crawling baby\": 117,\n    \"crossing eyes\": 118,\n    \"crossing river\": 119,\n    \"crying\": 120,\n    \"cumbia\": 121,\n    \"curling (sport)\": 122,\n    \"curling hair\": 123,\n    \"cutting apple\": 124,\n    \"cutting nails\": 125,\n    \"cutting orange\": 126,\n    \"cutting pineapple\": 127,\n    \"cutting watermelon\": 128,\n    \"dancing ballet\": 129,\n    \"dancing charleston\": 130,\n    \"dancing gangnam style\": 131,\n    \"dancing macarena\": 132,\n    \"deadlifting\": 133,\n    \"decorating the christmas tree\": 134,\n    \"delivering mail\": 135,\n    \"dining\": 136,\n    \"directing traffic\": 137,\n    \"disc golfing\": 138,\n    \"diving cliff\": 139,\n    \"docking boat\": 140,\n    \"dodgeball\": 141,\n    \"doing aerobics\": 142,\n    \"doing jigsaw puzzle\": 143,\n    \"doing laundry\": 144,\n    \"doing nails\": 145,\n    \"drawing\": 146,\n    \"dribbling basketball\": 147,\n    \"drinking shots\": 148,\n    \"driving car\": 149,\n    \"driving tractor\": 150,\n    \"drooling\": 151,\n    \"drop kicking\": 152,\n    \"drumming fingers\": 153,\n    \"dumpster diving\": 154,\n    \"dunking basketball\": 155,\n    \"dyeing eyebrows\": 156,\n    \"dyeing hair\": 157,\n    \"eating burger\": 158,\n    \"eating cake\": 159,\n    \"eating carrots\": 160,\n    \"eating chips\": 161,\n    \"eating doughnuts\": 162,\n    \"eating hotdog\": 163,\n    \"eating ice cream\": 164,\n    \"eating spaghetti\": 165,\n    \"eating watermelon\": 166,\n    \"egg hunting\": 167,\n    \"embroidering\": 168,\n    \"exercising with an exercise ball\": 169,\n    \"extinguishing fire\": 170,\n    \"faceplanting\": 171,\n    \"falling off bike\": 172,\n    \"falling off chair\": 173,\n    \"feeding birds\": 174,\n    \"feeding fish\": 175,\n    \"feeding goats\": 176,\n    \"fencing (sport)\": 177,\n    \"fidgeting\": 178,\n    \"finger snapping\": 179,\n    \"fixing bicycle\": 180,\n    \"fixing hair\": 181,\n    \"flint knapping\": 182,\n    \"flipping pancake\": 183,\n    \"fly tying\": 184,\n    \"flying kite\": 185,\n    \"folding clothes\": 186,\n    \"folding napkins\": 187,\n    \"folding paper\": 188,\n    \"front raises\": 189,\n    \"frying vegetables\": 190,\n    \"geocaching\": 191,\n    \"getting a haircut\": 192,\n    \"getting a piercing\": 193,\n    \"getting a tattoo\": 194,\n    \"giving or receiving award\": 195,\n    \"gold panning\": 196,\n    \"golf chipping\": 197,\n    \"golf driving\": 198,\n    \"golf putting\": 199,\n    \"gospel singing in church\": 200,\n    \"grinding meat\": 201,\n    \"grooming dog\": 202,\n    \"grooming horse\": 203,\n    \"gymnastics tumbling\": 204,\n    \"hammer throw\": 205,\n    \"hand washing clothes\": 206,\n    \"head stand\": 207,\n    \"headbanging\": 208,\n    \"headbutting\": 209,\n    \"high jump\": 210,\n    \"high kick\": 211,\n    \"historical reenactment\": 212,\n    \"hitting baseball\": 213,\n    \"hockey stop\": 214,\n    \"holding snake\": 215,\n    \"home roasting coffee\": 216,\n    \"hopscotch\": 217,\n    \"hoverboarding\": 218,\n    \"huddling\": 219,\n    \"hugging (not baby)\": 220,\n    \"hugging baby\": 221,\n    \"hula hooping\": 222,\n    \"hurdling\": 223,\n    \"hurling (sport)\": 224,\n    \"ice climbing\": 225,\n    \"ice fishing\": 226,\n    \"ice skating\": 227,\n    \"ice swimming\": 228,\n    \"inflating balloons\": 229,\n    \"installing carpet\": 230,\n    \"ironing\": 231,\n    \"ironing hair\": 232,\n    \"javelin throw\": 233,\n    \"jaywalking\": 234,\n    \"jetskiing\": 235,\n    \"jogging\": 236,\n    \"juggling balls\": 237,\n    \"juggling fire\": 238,\n    \"juggling soccer ball\": 239,\n    \"jumping bicycle\": 240,\n    \"jumping into pool\": 241,\n    \"jumping jacks\": 242,\n    \"jumpstyle dancing\": 243,\n    \"karaoke\": 244,\n    \"kicking field goal\": 245,\n    \"kicking soccer ball\": 246,\n    \"kissing\": 247,\n    \"kitesurfing\": 248,\n    \"knitting\": 249,\n    \"krumping\": 250,\n    \"land sailing\": 251,\n    \"laughing\": 252,\n    \"lawn mower racing\": 253,\n    \"laying bricks\": 254,\n    \"laying concrete\": 255,\n    \"laying stone\": 256,\n    \"laying tiles\": 257,\n    \"leatherworking\": 258,\n    \"licking\": 259,\n    \"lifting hat\": 260,\n    \"lighting fire\": 261,\n    \"lock picking\": 262,\n    \"long jump\": 263,\n    \"longboarding\": 264,\n    \"looking at phone\": 265,\n    \"luge\": 266,\n    \"lunge\": 267,\n    \"making a cake\": 268,\n    \"making a sandwich\": 269,\n    \"making balloon shapes\": 270,\n    \"making bubbles\": 271,\n    \"making cheese\": 272,\n    \"making horseshoes\": 273,\n    \"making jewelry\": 274,\n    \"making paper aeroplanes\": 275,\n    \"making pizza\": 276,\n    \"making snowman\": 277,\n    \"making sushi\": 278,\n    \"making tea\": 279,\n    \"making the bed\": 280,\n    \"marching\": 281,\n    \"marriage proposal\": 282,\n    \"massaging back\": 283,\n    \"massaging feet\": 284,\n    \"massaging legs\": 285,\n    \"massaging neck\": 286,\n    \"massaging person's head\": 287,\n    \"milking cow\": 288,\n    \"moon walking\": 289,\n    \"mopping floor\": 290,\n    \"mosh pit dancing\": 291,\n    \"motorcycling\": 292,\n    \"mountain climber (exercise)\": 293,\n    \"moving furniture\": 294,\n    \"mowing lawn\": 295,\n    \"mushroom foraging\": 296,\n    \"needle felting\": 297,\n    \"news anchoring\": 298,\n    \"opening bottle (not wine)\": 299,\n    \"opening door\": 300,\n    \"opening present\": 301,\n    \"opening refrigerator\": 302,\n    \"opening wine bottle\": 303,\n    \"packing\": 304,\n    \"paragliding\": 305,\n    \"parasailing\": 306,\n    \"parkour\": 307,\n    \"passing American football (in game)\": 308,\n    \"passing american football (not in game)\": 309,\n    \"passing soccer ball\": 310,\n    \"peeling apples\": 311,\n    \"peeling potatoes\": 312,\n    \"person collecting garbage\": 313,\n    \"petting animal (not cat)\": 314,\n    \"petting cat\": 315,\n    \"photobombing\": 316,\n    \"photocopying\": 317,\n    \"picking fruit\": 318,\n    \"pillow fight\": 319,\n    \"pinching\": 320,\n    \"pirouetting\": 321,\n    \"planing wood\": 322,\n    \"planting trees\": 323,\n    \"plastering\": 324,\n    \"playing accordion\": 325,\n    \"playing badminton\": 326,\n    \"playing bagpipes\": 327,\n    \"playing basketball\": 328,\n    \"playing bass guitar\": 329,\n    \"playing beer pong\": 330,\n    \"playing blackjack\": 331,\n    \"playing cello\": 332,\n    \"playing chess\": 333,\n    \"playing clarinet\": 334,\n    \"playing controller\": 335,\n    \"playing cricket\": 336,\n    \"playing cymbals\": 337,\n    \"playing darts\": 338,\n    \"playing didgeridoo\": 339,\n    \"playing dominoes\": 340,\n    \"playing drums\": 341,\n    \"playing field hockey\": 342,\n    \"playing flute\": 343,\n    \"playing gong\": 344,\n    \"playing guitar\": 345,\n    \"playing hand clapping games\": 346,\n    \"playing harmonica\": 347,\n    \"playing harp\": 348,\n    \"playing ice hockey\": 349,\n    \"playing keyboard\": 350,\n    \"playing kickball\": 351,\n    \"playing laser tag\": 352,\n    \"playing lute\": 353,\n    \"playing maracas\": 354,\n    \"playing marbles\": 355,\n    \"playing monopoly\": 356,\n    \"playing netball\": 357,\n    \"playing ocarina\": 358,\n    \"playing organ\": 359,\n    \"playing paintball\": 360,\n    \"playing pan pipes\": 361,\n    \"playing piano\": 362,\n    \"playing pinball\": 363,\n    \"playing ping pong\": 364,\n    \"playing poker\": 365,\n    \"playing polo\": 366,\n    \"playing recorder\": 367,\n    \"playing rubiks cube\": 368,\n    \"playing saxophone\": 369,\n    \"playing scrabble\": 370,\n    \"playing squash or racquetball\": 371,\n    \"playing tennis\": 372,\n    \"playing trombone\": 373,\n    \"playing trumpet\": 374,\n    \"playing ukulele\": 375,\n    \"playing violin\": 376,\n    \"playing volleyball\": 377,\n    \"playing with trains\": 378,\n    \"playing xylophone\": 379,\n    \"poking bellybutton\": 380,\n    \"pole vault\": 381,\n    \"polishing metal\": 382,\n    \"popping balloons\": 383,\n    \"pouring beer\": 384,\n    \"preparing salad\": 385,\n    \"presenting weather forecast\": 386,\n    \"pull ups\": 387,\n    \"pumping fist\": 388,\n    \"pumping gas\": 389,\n    \"punching bag\": 390,\n    \"punching person (boxing)\": 391,\n    \"push up\": 392,\n    \"pushing car\": 393,\n    \"pushing cart\": 394,\n    \"pushing wheelbarrow\": 395,\n    \"pushing wheelchair\": 396,\n    \"putting in contact lenses\": 397,\n    \"putting on eyeliner\": 398,\n    \"putting on foundation\": 399,\n    \"putting on lipstick\": 400,\n    \"putting on mascara\": 401,\n    \"putting on sari\": 402,\n    \"putting on shoes\": 403,\n    \"raising eyebrows\": 404,\n    \"reading book\": 405,\n    \"reading newspaper\": 406,\n    \"recording music\": 407,\n    \"repairing puncture\": 408,\n    \"riding a bike\": 409,\n    \"riding camel\": 410,\n    \"riding elephant\": 411,\n    \"riding mechanical bull\": 412,\n    \"riding mule\": 413,\n    \"riding or walking with horse\": 414,\n    \"riding scooter\": 415,\n    \"riding snow blower\": 416,\n    \"riding unicycle\": 417,\n    \"ripping paper\": 418,\n    \"roasting marshmallows\": 419,\n    \"roasting pig\": 420,\n    \"robot dancing\": 421,\n    \"rock climbing\": 422,\n    \"rock scissors paper\": 423,\n    \"roller skating\": 424,\n    \"rolling pastry\": 425,\n    \"rope pushdown\": 426,\n    \"running on treadmill\": 427,\n    \"sailing\": 428,\n    \"salsa dancing\": 429,\n    \"sanding floor\": 430,\n    \"sausage making\": 431,\n    \"sawing wood\": 432,\n    \"scrambling eggs\": 433,\n    \"scrapbooking\": 434,\n    \"scrubbing face\": 435,\n    \"scuba diving\": 436,\n    \"separating eggs\": 437,\n    \"setting table\": 438,\n    \"sewing\": 439,\n    \"shaking hands\": 440,\n    \"shaking head\": 441,\n    \"shaping bread dough\": 442,\n    \"sharpening knives\": 443,\n    \"sharpening pencil\": 444,\n    \"shaving head\": 445,\n    \"shaving legs\": 446,\n    \"shearing sheep\": 447,\n    \"shining flashlight\": 448,\n    \"shining shoes\": 449,\n    \"shooting basketball\": 450,\n    \"shooting goal (soccer)\": 451,\n    \"shopping\": 452,\n    \"shot put\": 453,\n    \"shoveling snow\": 454,\n    \"shucking oysters\": 455,\n    \"shuffling cards\": 456,\n    \"shuffling feet\": 457,\n    \"side kick\": 458,\n    \"sign language interpreting\": 459,\n    \"singing\": 460,\n    \"sipping cup\": 461,\n    \"situp\": 462,\n    \"skateboarding\": 463,\n    \"ski jumping\": 464,\n    \"skiing crosscountry\": 465,\n    \"skiing mono\": 466,\n    \"skiing slalom\": 467,\n    \"skipping rope\": 468,\n    \"skipping stone\": 469,\n    \"skydiving\": 470,\n    \"slacklining\": 471,\n    \"slapping\": 472,\n    \"sled dog racing\": 473,\n    \"sleeping\": 474,\n    \"smashing\": 475,\n    \"smelling feet\": 476,\n    \"smoking\": 477,\n    \"smoking hookah\": 478,\n    \"smoking pipe\": 479,\n    \"snatch weight lifting\": 480,\n    \"sneezing\": 481,\n    \"snorkeling\": 482,\n    \"snowboarding\": 483,\n    \"snowkiting\": 484,\n    \"snowmobiling\": 485,\n    \"somersaulting\": 486,\n    \"spelunking\": 487,\n    \"spinning poi\": 488,\n    \"spray painting\": 489,\n    \"springboard diving\": 490,\n    \"square dancing\": 491,\n    \"squat\": 492,\n    \"standing on hands\": 493,\n    \"staring\": 494,\n    \"steer roping\": 495,\n    \"sticking tongue out\": 496,\n    \"stomping grapes\": 497,\n    \"stretching arm\": 498,\n    \"stretching leg\": 499,\n    \"sucking lolly\": 500,\n    \"surfing crowd\": 501,\n    \"surfing water\": 502,\n    \"sweeping floor\": 503,\n    \"swimming backstroke\": 504,\n    \"swimming breast stroke\": 505,\n    \"swimming butterfly stroke\": 506,\n    \"swimming front crawl\": 507,\n    \"swing dancing\": 508,\n    \"swinging baseball bat\": 509,\n    \"swinging on something\": 510,\n    \"sword fighting\": 511,\n    \"sword swallowing\": 512,\n    \"tackling\": 513,\n    \"tagging graffiti\": 514,\n    \"tai chi\": 515,\n    \"talking on cell phone\": 516,\n    \"tango dancing\": 517,\n    \"tap dancing\": 518,\n    \"tapping guitar\": 519,\n    \"tapping pen\": 520,\n    \"tasting beer\": 521,\n    \"tasting food\": 522,\n    \"tasting wine\": 523,\n    \"testifying\": 524,\n    \"texting\": 525,\n    \"threading needle\": 526,\n    \"throwing axe\": 527,\n    \"throwing ball (not baseball or American football)\": 528,\n    \"throwing discus\": 529,\n    \"throwing knife\": 530,\n    \"throwing snowballs\": 531,\n    \"throwing tantrum\": 532,\n    \"throwing water balloon\": 533,\n    \"tickling\": 534,\n    \"tie dying\": 535,\n    \"tightrope walking\": 536,\n    \"tiptoeing\": 537,\n    \"tobogganing\": 538,\n    \"tossing coin\": 539,\n    \"training dog\": 540,\n    \"trapezing\": 541,\n    \"trimming or shaving beard\": 542,\n    \"trimming shrubs\": 543,\n    \"trimming trees\": 544,\n    \"triple jump\": 545,\n    \"twiddling fingers\": 546,\n    \"tying bow tie\": 547,\n    \"tying knot (not on a tie)\": 548,\n    \"tying necktie\": 549,\n    \"tying shoe laces\": 550,\n    \"unboxing\": 551,\n    \"unloading truck\": 552,\n    \"using a microscope\": 553,\n    \"using a paint roller\": 554,\n    \"using a power drill\": 555,\n    \"using a sledge hammer\": 556,\n    \"using a wrench\": 557,\n    \"using atm\": 558,\n    \"using bagging machine\": 559,\n    \"using circular saw\": 560,\n    \"using inhaler\": 561,\n    \"using puppets\": 562,\n    \"using remote controller (not gaming)\": 563,\n    \"using segway\": 564,\n    \"vacuuming floor\": 565,\n    \"visiting the zoo\": 566,\n    \"wading through mud\": 567,\n    \"wading through water\": 568,\n    \"waiting in line\": 569,\n    \"waking up\": 570,\n    \"walking the dog\": 571,\n    \"walking through snow\": 572,\n    \"washing dishes\": 573,\n    \"washing feet\": 574,\n    \"washing hair\": 575,\n    \"washing hands\": 576,\n    \"watching tv\": 577,\n    \"water skiing\": 578,\n    \"water sliding\": 579,\n    \"watering plants\": 580,\n    \"waving hand\": 581,\n    \"waxing back\": 582,\n    \"waxing chest\": 583,\n    \"waxing eyebrows\": 584,\n    \"waxing legs\": 585,\n    \"weaving basket\": 586,\n    \"weaving fabric\": 587,\n    \"welding\": 588,\n    \"whistling\": 589,\n    \"windsurfing\": 590,\n    \"winking\": 591,\n    \"wood burning (art)\": 592,\n    \"wrapping present\": 593,\n    \"wrestling\": 594,\n    \"writing\": 595,\n    \"yarn spinning\": 596,\n    \"yawning\": 597,\n    \"yoga\": 598,\n    \"zumba\": 599\n  },\n  \"layer_norm_eps\": 1e-06,\n  \"model_type\": \"timesformer\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_frames\": 8,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"qkv_bias\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.46.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"add_cross_attention\": true,\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"is_decoder\": true,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport os\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport shutil\n\n# Function to train the model\ndef train_model(model, train_dataloader, optimizer, num_epochs=5, checkpoint_dir=\"/kaggle/working/checkpoints2\"):\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    best_accuracy = 0  # Initialize the best accuracy as 0\n    best_checkpoint_path = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        \n        # Using tqdm for progress bar\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n            pixel_values, labels = batch[:2]  # Only use the first two components\n            # Proceed with training logic\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n        # Evaluate model and generate captions after every epoch\n        if (epoch + 1) % 3 == 0:  # Evaluate every 2 epochs\n\n            \n            torch.save(model.state_dict(), \"/kaggle/working/epoch\"+str(epoch)+\".pth\")\n\n            # checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}accuracy{best_accuracy:.4f}.bin\")\n            # torch.save(model.state_dict(), checkpoint_path)\n            # print(f\"New best model saved at: {checkpoint_path}\")\n            \n            # # Remove previous checkpoint if exists\n            # if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n            #     os.remove(best_checkpoint_path)\n            \n            # best_checkpoint_path = checkpoint_path\n\n            \n            print(\"\\nGenerating captions for a random video from the test set:\")\n            random_video_idx = random.choice(range(len(test_dataloader.dataset)))\n            batch = test_dataloader.dataset[random_video_idx]\n            pixel_values, labels = batch[:2]\n            generate_caption(model, pixel_values)\n            \n\n# Function to evaluate the model\ndef evaluate_model(model, test_dataloader):\n    model.eval()\n    gen_kwargs = {\n        \"min_length\": 10,\n        \"max_length\": 20,\n        \"num_beams\": 8,\n    }\n    true_labels = []\n    predicted_labels = []\n\n    with torch.no_grad():\n        for pixel_values, labels in test_dataloader:\n            # Generate captions\n            tokens = model.generate(pixel_values, **gen_kwargs)\n            captions = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n            \n            # Assuming the labels are text and we compare them directly for accuracy\n            true_labels.append(labels)\n            predicted_labels.append(captions)\n\n            # We break after the first batch for now\n            break\n\n    # Assuming the labels and predictions are text, we can calculate accuracy (this can vary based on your actual task)\n    accuracy = calculate_accuracy(true_labels, predicted_labels)\n    print(f\"Model accuracy: {accuracy:.4f}\")\n    return accuracy\n\n\n# Function to generate captions on a random video\ndef generate_caption(model, pixel_values):\n    model.eval()\n    gen_kwargs = {\n        \"min_length\": 10,\n        \"max_length\": 20,\n        \"num_beams\": 8,\n    }\n\n    with torch.no_grad():\n        tokens = model.generate(pixel_values.unsqueeze(0), **gen_kwargs)  # Add batch dimension\n        caption = tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n        print(f\"Generated Caption: {caption}\")\n\n# Initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Start the training process\ntrain_model(model, train_dataloader, optimizer, num_epochs=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:48:52.993731Z","iopub.execute_input":"2024-12-08T16:48:52.994397Z"}},"outputs":[{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/30], Loss: 2.2821\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/30], Loss: 1.7652\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/30], Loss: 1.4696\n\nGenerating captions for a random video from the test set:\n","output_type":"stream"},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Generated Caption: Cesc Fabregas (Chelsea) sends a cross into the box, but one of the\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/30], Loss: 1.2481\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/30], Loss: 1.0796\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/30], Loss: 0.9520\n\nGenerating captions for a random video from the test set:\nGenerated Caption: The referee stops play so that a substitution can be made and Aleksandar Mitrovic\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/30], Loss: 0.8188\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/30], Loss: 0.6976\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/30], Loss: 0.6272\n\nGenerating captions for a random video from the test set:\nGenerated Caption: Cesc Fabregas (Chelsea) floats the free kick into the box, but it's\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/30], Loss: 0.5605\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/30], Loss: 0.5113\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/30], Loss: 0.4575\n\nGenerating captions for a random video from the test set:\nGenerated Caption: Pedro (Chelsea) races towards goal but the defender gets back well to make a challenge.\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/30], Loss: 0.4232\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/30], Loss: 0.3837\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/30], Loss: 0.3548\n\nGenerating captions for a random video from the test set:\nGenerated Caption: A yellow card for a tackle by Mark Noble (West Ham). Andy Carroll (West Ham)\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/30], Loss: 0.3277\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/30], Loss: 0.3152\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/30], Loss: 0.3023\n\nGenerating captions for a random video from the test set:\nGenerated Caption: Eden Hazard (Chelsea) produces a killer pass onto Diego Costa, who loses the ball to\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/30], Loss: 0.2804\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/30], Loss: 0.2603\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [21/30], Loss: 0.2393\n\nGenerating captions for a random video from the test set:\nGenerated Caption: A player from Southampton takes his opponent down, the referee blows his whistle for a foul. It\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [22/30], Loss: 0.2476\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [23/30], Loss: 0.2342\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [24/30], Loss: 0.2288\n\nGenerating captions for a random video from the test set:\nGenerated Caption: Cesc Fabregas (Chelsea) floats the ball in from the corner but it's intercepted\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [25/30], Loss: 0.2120\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [26/30], Loss: 0.2096\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [27/30], Loss: 0.1891\n\nGenerating captions for a random video from the test set:\nGenerated Caption: Goal! Sadio Mane displays great vision and sends a pass to Graziano Pelle (\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [28/30], Loss: 0.1889\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [29/30], Loss: 0.1770\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 23/62 [01:22<02:18,  3.55s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:49:04.772408Z","iopub.execute_input":"2024-12-08T11:49:04.773142Z","iopub.status.idle":"2024-12-08T11:49:13.783180Z","shell.execute_reply.started":"2024-12-08T11:49:04.773108Z","shell.execute_reply":"2024-12-08T11:49:13.781939Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:59:18.731631Z","iopub.execute_input":"2024-12-08T11:59:18.732026Z","iopub.status.idle":"2024-12-08T11:59:29.406353Z","shell.execute_reply.started":"2024-12-08T11:59:18.731977Z","shell.execute_reply":"2024-12-08T11:59:29.404895Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b8191a70f41d5d6ade303df35767220b7c3a6e15a2fed9c2f3b94628108b66ac\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport os\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport shutil\nimport evaluate\n\n# Initialize metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\n# Function to calculate Perplexity\ndef calculate_perplexity(loss):\n    return torch.exp(loss)\n\n# Function to train the model\ndef train_model(model, train_dataloader, optimizer, num_epochs=5, checkpoint_dir=\"checkpoints\"):\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    best_accuracy = 0  # Initialize the best accuracy as 0\n    best_checkpoint_path = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        \n        # Using tqdm for progress bar\n        for pixel_values, labels, text in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n\n            print(\"\\nGenerating captions for a random video from the test set:\")\n            random_video_idx = random.choice(range(len(test_dataloader.dataset)))\n            pixel_values, labels = train_dataloader.dataset[random_video_idx]\n            print(\"original: \",text)\n            caption = generate_caption(model, pixel_values)\n    \n            # Evaluate BLEU, ROUGE, and Perplexity\n            print(\"\\nEvaluating on the test set:\")\n            bleu_score, rouge_score, perplexity = evaluate_model(caption, text)\n            print(f\"BLEU score: {bleu_score:.4f}, ROUGE score: {rouge_score:.4f}, Perplexity: {perplexity:.4f}\")\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n        # Evaluate model and generate captions after every epoch\n        # if (epoch + 1) % 2 == 0:  # Evaluate every 2 epochs\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}_accuracy_{best_accuracy:.4f}.bin\")\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"New best model saved at: {checkpoint_path}\")\n        \n        # Remove previous checkpoint if exists\n        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n            os.remove(best_checkpoint_path)\n        \n        best_checkpoint_path = checkpoint_path\n\n        print(\"\\nGenerating captions for a random video from the test set:\")\n        random_video_idx = random.choice(range(len(test_dataloader.dataset)))\n        pixel_values, labels = train_dataloader.dataset[random_video_idx]\n        print(\"original: \",text)\n        caption = generate_caption(model, pixel_values)\n\n        # Evaluate BLEU, ROUGE, and Perplexity\n        print(\"\\nEvaluating on the test set:\")\n        bleu_score, rouge_score, perplexity = evaluate_model(caption, text)\n        print(f\"BLEU score: {bleu_score:.4f}, ROUGE score: {rouge_score:.4f}, Perplexity: {perplexity:.4f}\")\n\n# Function to evaluate the model\ndef evaluate_model(predicted_labels, true_labels):\n\n    # Flatten the lists if necessary\n    true_labels = [item for sublist in true_labels for item in sublist]\n    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n\n    # Compute BLEU and ROUGE scores\n    bleu_score = bleu_metric.compute(predictions=predicted_labels, references=true_labels)[\"bleu\"]\n    rouge_score = rouge_metric.compute(predictions=predicted_labels, references=true_labels)[\"rouge1\"]\n\n    # Calculate Perplexity\n    perplexity = calculate_perplexity(total_loss / len(test_dataloader))\n\n    return bleu_score, rouge_score, perplexity\n\n# Function to generate captions on a random video\ndef generate_caption(model, pixel_values):\n    model.eval()\n    gen_kwargs = {\n        \"min_length\": 10,\n        \"max_length\": 20,\n        \"num_beams\": 8,\n    }\n\n    with torch.no_grad():\n        tokens = model.generate(pixel_values.unsqueeze(0), **gen_kwargs)  # Add batch dimension\n        caption = tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n        print(f\"Generated Caption: {caption}\")\n    return caption\n\n# Initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Start the training process\ntrain_model(model, train_dataloader, optimizer, num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:34:05.477917Z","iopub.execute_input":"2024-12-08T12:34:05.478757Z","iopub.status.idle":"2024-12-08T12:46:02.923789Z","shell.execute_reply.started":"2024-12-08T12:34:05.478722Z","shell.execute_reply":"2024-12-08T12:46:02.922318Z"}},"outputs":[{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Loss: 0.5813\nNew best model saved at: checkpoints/model_epoch_1_accuracy_0.0000.bin\n\nGenerating captions for a random video from the test set:\noriginal:  Die\nGenerated Caption: Goal! Sadio Mane displays great vision and sends a pass to Graziano Pelle (\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5], Loss: 0.5006\nNew best model saved at: checkpoints/model_epoch_2_accuracy_0.0000.bin\n\nGenerating captions for a random video from the test set:\noriginal:  A\nGenerated Caption: A yellow card for a tackle by Vincent Kompany (Manchester City). Manuel Pellegr\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5], Loss: 0.4407\nNew best model saved at: checkpoints/model_epoch_3_accuracy_0.0000.bin\n\nGenerating captions for a random video from the test set:\noriginal:  The\nGenerated Caption: The referee stops play so that a substitution can be made and Aleksandar Mitrovic\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[29], line 31\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, optimizer, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Using tqdm for progress bar\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pixel_values, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mVideoCaptionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m container\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(container\u001b[38;5;241m.\u001b[39mdecode(video\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m     44\u001b[0m         frames\u001b[38;5;241m.\u001b[39mappend(frame\u001b[38;5;241m.\u001b[39mto_ndarray(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Prepare pixel values using the image processor\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"import torch\nimport os\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport shutil\nimport evaluate\n\n# Initialize metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\n# Function to calculate Perplexity\ndef calculate_perplexity(loss):\n    return torch.exp(loss)\n\n# Function to train the model\ndef train_model(model, train_dataloader, optimizer, num_epochs=5, checkpoint_dir=\"checkpoints\"):\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    best_accuracy = 0  # Initialize the best accuracy as 0\n    best_checkpoint_path = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        \n        # Using tqdm for progress bar\n        for pixel_values, labels, text in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n\n            loss = torch.Tensor([0.5])\n            print(\"\\nGenerating captions for a random video from the test set:\")\n            random_video_idx = random.choice(range(len(train_dataloader.dataset)))\n            pixel_values, labels, text = train_dataloader.dataset[random_video_idx]\n            print(\"original: \",text)\n            caption = generate_caption(model, pixel_values)\n    \n            # Evaluate BLEU, ROUGE, and Perplexity\n            print(\"\\nEvaluating on the test set:\")\n            bleu_score, rouge_score, perplexity = evaluate_model(loss, caption, text)\n            print(f\"BLEU score: {bleu_score}, ROUGE score: {rouge_score}, Perplexity: {perplexity}\")\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n        # Evaluate model and generate captions after every epoch\n        # if (epoch + 1) % 2 == 0:  # Evaluate every 2 epochs\n        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}_accuracy_{best_accuracy:.4f}.bin\")\n        torch.save(model.state_dict(), checkpoint_path)\n        print(f\"New best model saved at: {checkpoint_path}\")\n        \n        # Remove previous checkpoint if exists\n        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n            os.remove(best_checkpoint_path)\n        \n        best_checkpoint_path = checkpoint_path\n\n        print(\"\\nGenerating captions for a random video from the test set:\")\n        random_video_idx = random.choice(range(len(test_dataloader.dataset)))\n        pixel_values, labels = train_dataloader.dataset[random_video_idx]\n        print(\"original: \",text)\n        caption = generate_caption(model, pixel_values)\n\n        # Evaluate BLEU, ROUGE, and Perplexity\n        print(\"\\nEvaluating on the test set:\")\n        bleu_score, rouge_score, perplexity = evaluate_model(caption, text)\n        print(f\"BLEU score: {bleu_score:.4f}, ROUGE score: {rouge_score:.4f}, Perplexity: {perplexity:.4f}\")\n\n# Function to evaluate the model\ndef evaluate_model(total_loss, predicted_labels, true_labels):\n\n    # Flatten the lists if necessary\n    true_labels = [item for sublist in true_labels for item in sublist]\n    predicted_labels = [item for sublist in predicted_labels for item in sublist][:len(true_labels)]\n    true_labels = [item for sublist in true_labels for item in sublist][:len(predicted_labels)]\n    \n\n    # Compute BLEU and ROUGE scores\n    bleu_score = bleu_metric.compute(predictions=predicted_labels, references=true_labels)[\"bleu\"]\n    rouge_score = rouge_metric.compute(predictions=predicted_labels, references=true_labels)[\"rouge1\"]\n\n    # Calculate Perplexity\n    perplexity = calculate_perplexity(total_loss / len(test_dataloader))\n\n    return bleu_score, rouge_score, perplexity\n\n# Function to generate captions on a random video\ndef generate_caption(model, pixel_values):\n    model.eval()\n    gen_kwargs = {\n        \"min_length\": 10,\n        \"max_length\": 40,\n        \"num_beams\": 8,\n    }\n\n    with torch.no_grad():\n        tokens = model.generate(pixel_values.unsqueeze(0), **gen_kwargs)  # Add batch dimension\n        caption = tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n        print(f\"Generated Caption: {caption}\")\n    return caption\n\n# Initialize optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Start the training process\ntrain_model(model, train_dataloader, optimizer, num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:06:18.116720Z","iopub.execute_input":"2024-12-08T13:06:18.117092Z","iopub.status.idle":"2024-12-08T13:06:27.794864Z","shell.execute_reply.started":"2024-12-08T13:06:18.117060Z","shell.execute_reply":"2024-12-08T13:06:27.793339Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5:   0%|          | 0/62 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nGenerating captions for a random video from the test set:\noriginal:  What a bad luck! Tyler Blackett (Manchester United) got in the way of a pass and unintentionally sent the ball past his own goalkeeper. The score is 1:1.\nGenerated Caption: Nacho Monreal (Arsenal) sends a lofted cross into the penalty area. Unfortunately for him, Mesut Ozil (Arsenal) found some space to play in on the edge of the\n\nEvaluating on the test set:\n","output_type":"stream"},{"name":"stderr","text":"                                                 ","output_type":"stream"},{"name":"stdout","text":"BLEU score: 0.0, ROUGE score: 0.026143790849673203, Perplexity: tensor([1.0317])\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[56], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, optimizer, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     50\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:592\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 592\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    600\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:634\u001b[0m, in \u001b[0;36mTimesformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    630\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 634\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    637\u001b[0m     embedding_output,\n\u001b[1;32m    638\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    639\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    640\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    641\u001b[0m )\n\u001b[1;32m    642\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:98\u001b[0m, in \u001b[0;36mTimesformerEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     95\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# create patch embeddings\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m embeddings, num_frames, patch_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, embeddings), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/timesformer/modeling_timesformer.py:60\u001b[0m, in \u001b[0;36mTimesformerPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[0;32m---> 60\u001b[0m     batch_size, num_frames, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     61\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_frames, num_channels, height, width)\n\u001b[1;32m     63\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(pixel_values)\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 5, got 4)","output_type":"error"}],"execution_count":56},{"cell_type":"code","source":"# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load pretrained processor, tokenizer, and model\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"Neleac/timesformer-gpt2-video-captioning\").to(device)\n\n# Freeze encoder parameters\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n\n# Define optimizer and loss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\n# Load and process video\nvideo_path = \"clip.mp4\"\ncontainer = av.open(video_path)\nseg_len = container.streams.video[0].frames\nclip_len = model.config.encoder.num_frames\nindices = set(np.linspace(0, seg_len, num=clip_len, endpoint=False).astype(np.int64))\nframes = []\n\ncontainer.seek(0)\nfor i, frame in enumerate(container.decode(video=0)):\n    if i in indices:\n        frames.append(frame.to_ndarray(format=\"rgb24\"))\n\n# Prepare pixel values\npixel_values = image_processor(frames, return_tensors=\"pt\").pixel_values.to(device)\n\n# Example training captions\ntrain_captions = [\"A man and a woman are dancing on a stage in front of a mirror.\"]\n\n# Assign a padding token if not defined\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize captions with padding and truncation\ninputs = tokenizer(train_captions, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True)\nlabels = inputs.input_ids.to(device)\n\n# Mask padding tokens in labels\nlabels[labels == tokenizer.pad_token_id] = -100\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = model(pixel_values, labels=labels)\n    loss = outputs.loss\n\n    # Backward pass and optimization\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n# Evaluate and generate captions\nmodel.eval()\ngen_kwargs = {\n    \"min_length\": 10,\n    \"max_length\": 20,\n    \"num_beams\": 8,\n}\nwith torch.no_grad():\n    tokens = model.generate(pixel_values, **gen_kwargs)\n    caption = tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n    print(f\"Generated Caption: {caption}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}